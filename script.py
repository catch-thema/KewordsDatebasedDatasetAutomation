import time
import urllib.parse
import re
from datetime import datetime, timedelta
import os
import sys
from collections import defaultdict
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import kss

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.common.exceptions import (
    TimeoutException,
    NoSuchElementException,
    WebDriverException
)
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

import openai
import pandas as pd
import numpy as np
import pandas_ta_classic as pdt
from pykrx import stock

OPENAI_API_KEY = ""

THEME_DB = defaultdict(list)
STOCK_TO_THEMES_DB = defaultdict(list)
TICKER_NAME_CACHE = {}
NAME_TICKER_CACHE = {}


def load_theme_db(csv_file="naver_themes_bs4.csv"):
    global THEME_DB, STOCK_TO_THEMES_DB
    try:
        df = pd.read_csv(csv_file, header=0)
        print(f"--- í…Œë§ˆ DB ë¡œë“œ ì¤‘: {csv_file} ---")
        theme_col = df.columns[0]

        for index, row in df.iterrows():
            theme_name = row[theme_col]
            if pd.isna(theme_name):
                continue
            stock_names = [name for name in row[1:].dropna() if name.strip()]
            THEME_DB[theme_name] = stock_names
            for name in stock_names:
                STOCK_TO_THEMES_DB[name].append(theme_name)
        print(f"âœ… í…Œë§ˆ DB ë¡œë“œ ì™„ë£Œ: ì´ {len(THEME_DB)}ê°œ í…Œë§ˆ, {len(STOCK_TO_THEMES_DB)}ê°œ ì¢…ëª© ë§¤í•‘")

    except FileNotFoundError:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: í…Œë§ˆ DB íŒŒì¼ '{csv_file}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit()
    except Exception as e:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: í…Œë§ˆ DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit()


def load_sentiment_model():
    print("--- KR-FinBert-SC ëª¨ë¸ ë¡œë“œ ì¤‘ (ìµœì´ˆ ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤) ---")
    try:
        model_name = "snunlp/KR-FinBert-SC"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)

        device_name = next(model.parameters()).device
        print(f"âœ… KR-FinBert-SC ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. (ì‹¤í–‰ ì¥ì¹˜: {device_name})")

        return tokenizer, model
    except Exception as e:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: KR-FinBert-SC ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit()


def calculate_finbert_sentiment(news_text, keywords, tokenizer, model):
    if not news_text or not keywords:
        return np.nan, []

    text = news_text
    text = re.sub(r'(Copyright Â© .*? All rights reserved\.?|ë¬´ë‹¨ ì „ì¬.*?ê¸ˆì§€\.?|Â© .*? ë¬´ë‹¨ ì „ì¬.*?)', '', text, flags=re.DOTALL)
    text = re.sub(r'\[.*? ì œê³µ\.?.*?\]', '', text)
    text = re.sub(r'\([ê°€-í£\w]+=[ê°€-í£\w]+\)', '', text)
    text = re.sub(r'\[.{1,15}\]$', '', text)
    text = re.sub(r'â–¶ .*? ë°”ë¡œê°€ê¸°|â–¶ .*? êµ¬ë…í•˜ê¸°|\s{0,}â“’ .*? ê¸ˆì§€', '', text)
    text = re.sub(r'[ê°€-í£\w]+ ê¸°ì \(.*?@.*?\..*?\)', '', text)
    text = re.sub(r'\([ê°€-í£\w]+=[ê°€-í£\w]+\) .*? ê¸°ì[)]?', '', text)
    text = re.sub(r'---\s?\[ê¸°ì‚¬ \d+\]\s?ì œëª©:', ' ', text, flags=re.IGNORECASE)
    text = re.sub(r'\[â“’.*?\]', '', text)
    text = re.sub(r'<ì €ì‘ê¶Œì>', '', text)
    text = re.sub(r'\s+', ' ', text).strip()

    try:
        sentences = kss.split_sentences(text)
    except Exception as e:
        print(f"âš ï¸ KSS ë¬¸ì¥ ë¶„ë¦¬ ì˜¤ë¥˜: {e}")
        return np.nan, []

    filtered_sentences = []
    for s in sentences:
        s_clean = s.strip()
        if len(s_clean) < 20:
            continue
        if any(kw in s_clean for kw in keywords if kw):
            filtered_sentences.append(s_clean)

    if not filtered_sentences:
        return np.nan, []

    try:
        label2id = model.config.label2id
        pos_id = label2id.get('positive', 2)
        neg_id = label2id.get('negative', 0)

        inputs = tokenizer(
            filtered_sentences,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=512
        )

        try:
            device = model.device
            inputs = {k: v.to(device) for k, v in inputs.items()}
        except Exception:
            pass

        with torch.no_grad():
            outputs = model(**inputs)

        probs = torch.softmax(outputs.logits, dim=-1)
        pos_probs = probs[:, pos_id].tolist()
        neg_probs = probs[:, neg_id].tolist()

        sentence_scores = []
        total_net_score = 0.0

        for i in range(len(filtered_sentences)):
            individual_score = pos_probs[i] - neg_probs[i]
            sentence_scores.append((filtered_sentences[i], individual_score))
            total_net_score += individual_score

    except Exception as e:
        print(f"âŒ FinBert ë°°ì¹˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return np.nan, []

    total_analyzed = len(sentence_scores)
    if total_analyzed == 0:
        return np.nan, []

    avg_score = total_net_score / total_analyzed

    return avg_score, sentence_scores


def parse_openai_theme_output(text, candidate_themes):
    try:
        cleaned_text = text.strip().replace("'", "").replace('"', "").replace(",", "")
        found_theme = None
        for theme in candidate_themes:
            if theme in cleaned_text:
                found_theme = theme
                break

        if not found_theme:
            first_line = cleaned_text.split('\n')[0].strip()
            if first_line in candidate_themes:
                found_theme = first_line
            else:
                parts = first_line.split()
                if parts and parts[0] in candidate_themes:
                    found_theme = parts[0]
                else:
                    print(f"âš ï¸ OpenAI í…Œë§ˆ íŒŒì‹± ê²½ê³ : ì‘ë‹µì—ì„œ ìœ íš¨í•œ í…Œë§ˆë¥¼ ì°¾ì§€ ëª»í•¨. '{text}'")
                    return None, np.nan

        remaining_text = cleaned_text.replace(found_theme, "").strip()
        match = re.search(r'[-+]?\d*\.?\d+', remaining_text)

        if match:
            try:
                scale_value = float(match.group(0))
                return found_theme, scale_value
            except ValueError:
                return found_theme, np.nan
        else:
            return found_theme, np.nan

    except Exception as e:
        print(f"âš ï¸ OpenAI í…Œë§ˆ/ê·œëª¨ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return None, np.nan


def call_openai_for_theme(news_content_text, leader_name, candidate_themes):
    max_text_length = 20000
    if len(news_content_text) > max_text_length:
        print(f"âš ï¸ OpenAI ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. {max_text_length}ìë¡œ ì œí•œí•©ë‹ˆë‹¤.")
        news_content_text = news_content_text[:max_text_length]

    if OPENAI_API_KEY == "":
        print("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None, np.nan
    try:
        client = openai.OpenAI(api_key=OPENAI_API_KEY)
    except Exception as e:
        print(f"âŒ OpenAI API ì„¤ì • ì˜¤ë¥˜: {e}")
        return None, np.nan

    system_instruction_text = (
        "You are a stock market theme analyzer. You will receive up to 10 news headlines, a lead stock ('ì£¼ë„ì£¼'), and a list of candidate themes ('í›„ë³´ í…Œë§ˆ ë¦¬ìŠ¤íŠ¸').\n\n"
        "Your tasks are:\n"
        "1. Select the **single most relevant theme** from the candidate list that the lead stock belongs to, based on the news headlines.\n"
        "2. Scan the headlines for any positive or negative financial figures related to the scale of transactions, contracts, sales, or investments.\n"
        "3. If multiple figures exist, select the one that *best* matches the theme you chose.\n"
        "4. You **must** convert this financial figure into units of **10 million KRW (ten million Korean Won)**.\n"
        "5. If the figure is negative (e.g., a loss, deficit, or fine), you **must** prepend a minus sign (-) to the number.\n\n"
        "Respond *only* with the theme name, followed by the number (if found). Do not add any other explanation.\n\n"
        "Example (Positive): A headline states 'Samsung achieved a 5 billion KRW contract.' (5,000,000,000 / 10,000,000 = 500). Your output: ë°˜ë„ì²´ 500\n"
        "Example (Negative): A headline states 'Company posts 300 million KRW operating loss.' (300,000,000 / 10,000,000 = 30). Your output: ì‹¤ì ë¶€ì§„ -30\n"
        "Example (No Number): If no relevant financial figure is found, respond *only* with the theme name. Your output: AI"
    )

    themes_as_string = ", ".join(candidate_themes)

    user_prompt = (
        f"Lead Stock: {leader_name}\n\n"
        f"--- News Headlines (up to 10) ---\n{news_content_text}\n\n"
        f"--- Candidate Themes ---\n[{themes_as_string}]\n\n"
        f"Select the single best theme and the corresponding financial figure (in 10M KRW units)."
    )

    print("\n--- OpenAI API í˜¸ì¶œ ì¤‘ (gpt-5-nano) ---")
    try:
        response = client.chat.completions.create(
            model="gpt-5-nano",
            messages=[
                {"role": "system", "content": system_instruction_text},
                {"role": "user", "content": user_prompt}
            ],
        )

        text_output = response.choices[0].message.content
        return parse_openai_theme_output(text_output, candidate_themes)

    except Exception as e:
        print(f"âŒ OpenAI API ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, np.nan


def get_ticker_by_name(name, base_date):
    global NAME_TICKER_CACHE

    cache_key = (name, base_date)
    if cache_key in NAME_TICKER_CACHE:
        return NAME_TICKER_CACHE[cache_key]

    try:
        tickers_df = stock.get_market_ticker_list(base_date, market="ALL")

        if not TICKER_NAME_CACHE.get(base_date):
            print(f"--- {base_date} ê¸°ì¤€ Ticker-Name ë§µ ìºì‹± ì¤‘... (ì‹œê°„ ì†Œìš”) ---")
            TICKER_NAME_CACHE[base_date] = {
                ticker: stock.get_market_ticker_name(ticker) for ticker in tickers_df
            }

        names_series = pd.Series(TICKER_NAME_CACHE[base_date])
        ticker = names_series[names_series == name].index[0]

        NAME_TICKER_CACHE[cache_key] = ticker
        return ticker
    except IndexError:
        NAME_TICKER_CACHE[cache_key] = None
        return None
    except Exception as e:
        print(f"âŒ pykrx Ticker ì¡°íšŒ ì˜¤ë¥˜: {name}, {e}")
        return None


def get_name_by_ticker(ticker, base_date):
    global TICKER_NAME_CACHE

    if base_date not in TICKER_NAME_CACHE:
        tickers_df = stock.get_market_ticker_list(base_date, market="ALL")
        TICKER_NAME_CACHE[base_date] = {
            ticker: stock.get_market_ticker_name(ticker) for ticker in tickers_df
        }
    return TICKER_NAME_CACHE[base_date].get(ticker, None)


def get_followers(theme, leader_name):
    if theme not in THEME_DB:
        return []
    followers = [name for name in THEME_DB[theme] if name != leader_name]
    return followers


def calculate_follower_features(ticker, end_date_ymd):
    if not ticker:
        return None, None, None, np.nan
    try:
        end_date = datetime.strptime(end_date_ymd, '%Y%m%d')
        start_date = (end_date - timedelta(days=100)).strftime('%Y%m%d')
        df = stock.get_market_ohlcv(start_date, end_date_ymd, ticker)
        if df.empty:
            return None, None, None, np.nan

        t_day_timestamp = pd.to_datetime(end_date_ymd)
        if t_day_timestamp not in df.index:
            t_day_timestamp = df.index[-1]
        actual_date_ymd = t_day_timestamp.strftime('%Y%m%d')
        today_data_for_close = df.loc[t_day_timestamp]
        t_day_close = today_data_for_close.get('ì¢…ê°€', np.nan)
    except Exception as e:
        return None, None, None, np.nan

    tech_features = {}
    try:
        df['RVOL5'] = df['ê±°ë˜ëŸ‰'] / df['ê±°ë˜ëŸ‰'].rolling(window=5, min_periods=1).mean()
        rsi_series = pdt.rsi(df['ì¢…ê°€'], length=9)
        macd_df = pdt.macd(df['ì¢…ê°€'], fast=12, slow=26, signal=15)
        sma_series = pdt.sma(df['ì¢…ê°€'], length=5)
        df['RSI_9'] = rsi_series
        df = pd.concat([df, macd_df], axis=1)
        df['MA5'] = sma_series
        df['Disparity5'] = (df['ì¢…ê°€'] / df['MA5']) * 100
        today_data = df.loc[t_day_timestamp]
        tech_features = {
            'í›„ë³´ì£¼_RSI_9': today_data.get('RSI_9', np.nan),
            'í›„ë³´ì£¼_MACDs_12_26_15': today_data.get('MACDs_12_26_15', np.nan),
            'í›„ë³´ì£¼_ì´ê²©ë„_5': today_data.get('Disparity5', np.nan),
            'í›„ë³´ì£¼_RVOL_5': today_data.get('RVOL5', np.nan)
        }
    except Exception as e:
        pass

    funda_features = {}
    try:
        f_df = stock.get_market_fundamental(actual_date_ymd, actual_date_ymd, ticker)
        if not f_df.empty:
            today_funda = f_df.iloc[0]
            funda_features = {
                'í›„ë³´ì£¼_DIV': today_funda.get('DIV', np.nan),
                'í›„ë³´ì£¼_BPS': today_funda.get('BPS', np.nan),
                'í›„ë³´ì£¼_PER': today_funda.get('PER', np.nan),
                'í›„ë³´ì£¼_EPS': today_funda.get('EPS', np.nan),
                'í›„ë³´ì£¼_PBR': today_funda.get('PBR', np.nan),
            }
    except Exception as e:
        pass

    return tech_features, funda_features, t_day_timestamp, t_day_close


def get_next_day_label(ticker, t_day_timestamp, t_day_close):
    if pd.isna(t_day_close) or t_day_timestamp is None:
        return np.nan
    try:
        start_search_date = (t_day_timestamp + timedelta(days=1)).strftime('%Y%m%d')
        end_search_date = (t_day_timestamp + timedelta(days=10)).strftime('%Y%m%d')
        df_future = stock.get_market_ohlcv(start_search_date, end_search_date, ticker)
        if df_future.empty:
            return np.nan
        t_plus_1_close = df_future.iloc[0]['ì¢…ê°€']

        # t_plus_1_close > t_day_close (ìƒìŠ¹=1)
        return 1 if t_plus_1_close > t_day_close else 0

    except Exception as e:
        return np.nan


def get_correlation(leader_ticker, follower_ticker, end_date_ymd, window=7):
    try:
        end_date = datetime.strptime(end_date_ymd, '%Y%m%d')
        start_date = (end_date - timedelta(days=window + 3)).strftime('%Y%m%d')
        end_date_minus_1 = (end_date - timedelta(days=1)).strftime('%Y%m%d')

        df_leader = stock.get_market_ohlcv(start_date, end_date_minus_1, leader_ticker)['ì¢…ê°€']
        df_follower = stock.get_market_ohlcv(start_date, end_date_minus_1, follower_ticker)['ì¢…ê°€']
        if df_leader.empty or df_follower.empty:
            return np.nan
        leader_ret = df_leader.pct_change().dropna()
        follower_ret = df_follower.pct_change().dropna()
        corr_df = pd.concat([leader_ret, follower_ret], axis=1, join='inner').dropna()
        if len(corr_df) < 3:
            return np.nan
        correlation = corr_df.iloc[:, 0].corr(corr_df.iloc[:, 1])
        return correlation
    except Exception as e:
        return np.nan


def get_theme_overlap(follower_name):
    if follower_name not in STOCK_TO_THEMES_DB:
        return 0, ""
    themes = STOCK_TO_THEMES_DB[follower_name]
    theme_count = len(themes)
    theme_list_str = ",".join(themes)
    return theme_count, theme_list_str


def calculate_leader_features(ticker, end_date_ymd):
    try:
        end_date = datetime.strptime(end_date_ymd, '%Y%m%d')
        start_date = (end_date - timedelta(days=100))
        start_date_ymd = start_date.strftime('%Y%m%d')

        df = stock.get_market_ohlcv(start_date_ymd, end_date_ymd, ticker)
        if df.empty:
            print(f"âŒ pykrx ì˜¤ë¥˜: {ticker}ì˜ ê°€ê²© ì •ë³´ë¥¼ Tì¼({end_date_ymd}) ê¸°ì¤€ìœ¼ë¡œ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        df['RVOL5'] = df['ê±°ë˜ëŸ‰'] / df['ê±°ë˜ëŸ‰'].rolling(window=5, min_periods=1).mean()
        rsi_series = pdt.rsi(df['ì¢…ê°€'], length=9)
        macd_df = pdt.macd(df['ì¢…ê°€'], fast=12, slow=26, signal=15)
        sma_series = pdt.sma(df['ì¢…ê°€'], length=5)
        df['RSI_9'] = rsi_series
        df = pd.concat([df, macd_df], axis=1)
        df['MA5'] = sma_series
        df['Disparity5'] = (df['ì¢…ê°€'] / df['MA5']) * 100
        df['Change_Pct'] = df['ì¢…ê°€'].pct_change() * 100

        t_day_timestamp = pd.to_datetime(end_date_ymd)
        if t_day_timestamp not in df.index:
            print(f"âš ï¸ ê²½ê³ : {end_date_ymd}ì€(ëŠ”) íœ´ì¥ì¼ì…ë‹ˆë‹¤. ì§ì „ ê±°ë˜ì¼ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            t_day_timestamp = df.index[-1]

        today_data = df.loc[t_day_timestamp]
        actual_date_ymd = t_day_timestamp.strftime('%Y%m%d')
        cap_df = stock.get_market_cap(actual_date_ymd, actual_date_ymd, ticker)
        market_cap = 0
        if not cap_df.empty:
            market_cap = cap_df.iloc[0]['ì‹œê°€ì´ì•¡']

        leader_features = {
            'ì£¼ë„ì£¼_ìƒìŠ¹ë¥ ': today_data.get('Change_Pct', 0.0),
            'ì£¼ë„ì£¼_ì‹œê°€ì´ì•¡': market_cap,
            'ì£¼ë„ì£¼_RSI_9': today_data.get('RSI_9', np.nan),
            'ì£¼ë„ì£¼_MACDs_12_26_15': today_data.get('MACDs_12_26_15', np.nan),
            'ì£¼ë„ì£¼_ì´ê²©ë„_5': today_data.get('Disparity5', np.nan),
            'ì£¼ë„ì£¼_RVOL_5': today_data.get('RVOL5', np.nan)
        }
        return leader_features

    except KeyError:
        return None
    except Exception as e:
        print(f"âŒ ì£¼ë„ì£¼ í”¼ì²˜ ê³„ì‚° ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        return None


def save_news_to_file(keyword, start_ymd, end_ymd, news_results):
    output_dir = "news_scraped"
    os.makedirs(output_dir, exist_ok=True)
    filename = os.path.join(output_dir, f"{keyword}_{start_ymd}_{end_ymd}.txt")
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"--- ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ---\n")
            f.write(f"í‚¤ì›Œë“œ: {keyword}\n")
            f.write(f"ê¸°ê°„: {start_ymd} ~ {end_ymd}\n")
            f.write(f"ì¶”ì¶œ ê°œìˆ˜: {len(news_results)}ê°œ\n\n")
            f.write("-" * 30 + "\n")
            for news in news_results:
                f.write(f"ì œëª©: {news['title']}\n")
                f.write(f"ë§í¬: {news.get('url', 'N/A')}\n")
                f.write(f"ë‚´ìš©: {news['summary']}\n")
                f.write("\n")
        print(f"âœ… ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
        return filename
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def save_sentiment_to_csv(keyword, follower_name, start_ymd, end_ymd, sentence_score_list, avg_score):
    if not sentence_score_list:
        return None

    output_dir = "sentiment_analyze"
    os.makedirs(output_dir, exist_ok=True)
    filename = os.path.join(output_dir, f"{keyword}_to_{follower_name}_{start_ymd}_{end_ymd}.csv")

    try:
        data_for_df = [{'ë¬¸ì¥': sent, 'ê°ì„±ì ìˆ˜': score} for sent, score in sentence_score_list]
        df = pd.DataFrame(data_for_df)

        avg_row = pd.DataFrame([{'ë¬¸ì¥': '--- ìµœì¢… í‰ê·  ---', 'ê°ì„±ì ìˆ˜': avg_score}])

        final_df = pd.concat([df, avg_row], ignore_index=True)

        final_df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… FinBert ë¶„ì„ ê²°ê³¼ ì €ì¥: {filename}")
        return filename
    except Exception as e:
        print(f"âŒ FinBert íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def initialize_driver():
    try:
        options = ChromeOptions()
        options.add_argument(
            'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        options.add_argument("lang=ko_KR")
        options.add_argument('--headless')
        options.add_argument('window-size=1920x1080')
        options.add_argument("disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("start-maximized")
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)

        driver.implicitly_wait(0)

        return driver
    except Exception as e:
        print(f"âŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None


def read_keywords_from_file(filename):
    data = []
    try:
        df = pd.read_csv(filename, encoding='utf-8')
        if df.empty:
            print(f"âš ï¸ ì…ë ¥ íŒŒì¼ '{filename}'ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return []
        if len(df.columns) < 2:
            print(f"âŒ ì˜¤ë¥˜: CSV íŒŒì¼ì— ìµœì†Œ 2ê°œì˜ ì—´('ì¢…ëª©ëª…', 'ë‚ ì§œ')ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return None

        col_keyword = df.columns[0]
        col_date = df.columns[1]
        print(f"--- ì…ë ¥ CSVì˜ '{col_keyword}'(ì¢…ëª©)ê³¼ '{col_date}'(ë‚ ì§œ) ì»¬ëŸ¼ì—ì„œ ë°ì´í„°ë¥¼ ì½ìŠµë‹ˆë‹¤. ---")

        for index, row in df.iterrows():
            keyword = row[col_keyword]
            date_val = row[col_date]
            if pd.isna(keyword) or pd.isna(date_val):
                continue
            keyword_str = str(keyword).strip()
            date_str = str(int(float(date_val))).strip()
            if keyword_str and re.fullmatch(r'\d{8}', date_str):
                data.append((keyword_str, date_str))
            elif keyword_str or date_str:
                print(f"âš ï¸ {index + 2}ë²ˆì§¸ í–‰ ìŠ¤í‚µ: í˜•ì‹ì´ 'ì¢…ëª©ëª…', 'YYYYMMDD'ì™€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. (ê°’: '{keyword_str}', '{date_str}')")
        return data
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: ì§€ì •ëœ íŒŒì¼ '{filename}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def scrape_naver_news_only_full_xpath(driver, keyword, start_date_ymd, end_date_ymd, max_count=10):
    links_to_visit = []
    results = []

    try:
        def format_date(ymd):
            return f"{ymd[:4]}-{ymd[4:6]}-{ymd[6:]}"

        start_date = format_date(start_date_ymd)
        end_date = format_date(end_date_ymd)
        encoded_keyword = urllib.parse.quote(keyword)
        start_date_dots = start_date.replace('-', '.')
        end_date_dots = end_date.replace('-', '.')
        url = (
            f"https://search.naver.com/search.naver?where=news"
            f"&query={encoded_keyword}"
            f"&sm=tab_opt&sort=0&photo=0&field=0&pd=3"
            f"&ds={start_date_dots}&de={end_date_dots}"
            f"&nso=so:r,p:from{start_date_ymd}to{end_date_ymd},a:all"
        )
        print(f"--- 1/2. ë§í¬ ìˆ˜ì§‘ ì‹œì‘: {url} ---")
        driver.get(url)

        wait = WebDriverWait(driver, 5)
        wait.until(EC.presence_of_element_located((By.CLASS_NAME, "group_news")))

        links_to_visit_set = set()
        last_link_count = 0
        scroll_attempts = 0

        while True:
            link_selector = "a[href*='n.news.naver.com/mnews/article']"
            news_link_elements = driver.find_elements(By.CSS_SELECTOR, link_selector)

            for el in news_link_elements:
                try:
                    href = el.get_attribute('href')
                    search_title = el.text

                    if href and search_title and (search_title, href) not in links_to_visit_set:
                        links_to_visit_set.add((search_title, href))
                except Exception:
                    pass

            if max_count is not None and max_count > 0 and len(links_to_visit_set) >= max_count:
                print(f"--- {max_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ. ìŠ¤í¬ë¡¤ ì¤‘ë‹¨. ---")
                break

            current_link_count = len(links_to_visit_set)
            if current_link_count == last_link_count:
                scroll_attempts += 1
            else:
                scroll_attempts = 0

            if scroll_attempts >= 3:
                print("--- í˜ì´ì§€ ëì— ë„ë‹¬. ìŠ¤í¬ë¡¤ ì¤‘ë‹¨. ---")
                break

            last_link_count = current_link_count

            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)
            time.sleep(0.8)

        links_to_visit = [{"search_title": title, "url": url} for title, url in links_to_visit_set]

        if max_count is not None and max_count > 0 and len(links_to_visit) > max_count:
            links_to_visit = links_to_visit[:max_count]

        print(f"--- 1/2. ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(links_to_visit)}ê±´ ---")

        wait = WebDriverWait(driver, 5)
        print(f"--- 2/2. ìˆ˜ì§‘ëœ ë§í¬ {len(links_to_visit)}ê°œ ë³¸ë¬¸/ì œëª© ìŠ¤í¬ë˜í•‘ ì‹œì‘ ---")

        for item in links_to_visit:
            try:
                driver.get(item['url'])
                body_text = ""
                article_title = ""

                user_title_xpath = "/html/body/div/div[2]/div/div[1]/div[1]/div[1]/div[2]/h2/span"
                common_title_class = "media_end_head_headline"

                try:
                    title_element = wait.until(EC.presence_of_element_located(
                        (By.XPATH, user_title_xpath)
                    ))
                    article_title = title_element.text
                except (TimeoutException, NoSuchElementException):
                    try:
                        title_element = driver.find_element(By.CLASS_NAME, common_title_class)
                        article_title = title_element.text
                    except NoSuchElementException:
                        print(f"âš ï¸ ë³¸ë¬¸ ì œëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ ì œëª©ì„ ëŒ€ì‹  ì‚¬ìš©í•©ë‹ˆë‹¤: {item['url']}")
                        article_title = item['search_title']

                user_body_xpath = "/html/body/div/div[2]/div/div[1]/div[1]/div[2]/div[1]/article"
                common_id_1 = "dic_area"
                common_id_2 = "contents"

                try:
                    body_element = wait.until(EC.presence_of_element_located(
                        (By.XPATH, user_body_xpath)
                    ))
                    body_text = body_element.text
                except (TimeoutException, NoSuchElementException):
                    try:
                        body_element = driver.find_element(By.ID, common_id_1)
                        body_text = body_element.text
                    except NoSuchElementException:
                        try:
                            body_element = driver.find_element(By.ID, common_id_2)
                            body_text = body_element.text
                        except NoSuchElementException:
                            print(f"âš ï¸ ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (XPath/ID ë¶ˆì¼ì¹˜): {item['url']}")
                            body_text = ""

                results.append({
                    "title": article_title,
                    "summary": body_text,
                    "url": item['url']
                })
                time.sleep(0.3)

            except Exception as e:
                print(f"âŒ ê¸°ì‚¬ í˜ì´ì§€({item['url']}) ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë˜í•‘ ì‘ì—… ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")

    print(f"--- ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: ì´ {len(results)}ê±´ ë³¸ë¬¸ ì¶”ì¶œ ---")

    return results


def save_intermediate_dataset(all_data_rows, output_filename):
    if not all_data_rows:
        print("--- ì¤‘ê°„ ì €ì¥: ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. ---")
        return

    try:
        print(f"\n--- ì¤‘ê°„ ì €ì¥ ì‹œì‘ (ì´ {len(all_data_rows)} í–‰) -> {output_filename} ---")

        final_df = pd.DataFrame(all_data_rows)

        theme_dummies = None
        if 'í…Œë§ˆ_ì¤‘ë³µ_ëª©ë¡' in final_df.columns:
            all_theme_names = list(THEME_DB.keys())
            theme_dummies = final_df['í…Œë§ˆ_ì¤‘ë³µ_ëª©ë¡'].str.get_dummies(sep=',')
            theme_dummies = theme_dummies.reindex(columns=all_theme_names, fill_value=0)
            theme_dummies = theme_dummies.add_prefix('í…Œë§ˆ_')
            final_df = pd.concat([final_df, theme_dummies], axis=1)

        columns_order = [
            'ì£¼ë„ì£¼', 'ì£¼ë„ì£¼_Ticker', 'T_Date', 'í…Œë§ˆ', 'ë‰´ìŠ¤ê¸°ì‚¬_ê·œëª¨ì ì§€ìˆ˜', 'ë‰´ìŠ¤ê¸°ì‚¬_ê°ì„±ì§€ìˆ˜',
            'ë‰´ìŠ¤_íŒŒì¼_ê²½ë¡œ', 'ì£¼ë„ì£¼_ìƒìŠ¹ë¥ ', 'ì£¼ë„ì£¼_ì‹œê°€ì´ì•¡', 'ì£¼ë„ì£¼_RSI_9', 'ì£¼ë„ì£¼_MACDs_12_26_15',
            'ì£¼ë„ì£¼_ì´ê²©ë„_5', 'ì£¼ë„ì£¼_RVOL_5', 'í›„ë³´ì£¼', 'í›„ë³´ì£¼_Ticker', 'í›„ë³´ì£¼_RSI_9',
            'í›„ë³´ì£¼_MACDs_12_26_15', 'í›„ë³´ì£¼_ì´ê²©ë„_5', 'í›„ë³´ì£¼_RVOL_5', 'í›„ë³´ì£¼_DIV', 'í›„ë³´ì£¼_BPS',
            'í›„ë³´ì£¼_PER', 'í›„ë³´ì£¼_EPS', 'í›„ë³´ì£¼_PBR',
            'ìƒê´€ê´€ê³„_ì§€ìˆ˜', 'í…Œë§ˆ_ì¤‘ë³µ_ê°œìˆ˜', 'Target'
        ]

        corrected_columns_order = []
        for col in columns_order:
            if col not in final_df.columns:
                if 'MACDs_12_26_15' in col:
                    alt_col = col.replace('MACDs_12_26_15', 'MACD_15s')
                    if alt_col in final_df.columns:
                        corrected_columns_order.append(alt_col)
                    else:
                        corrected_columns_order.append(col)
                elif 'MACD_15s' in col:
                    alt_col = col.replace('MACD_15s', 'MACDs_12_26_15')
                    if alt_col in final_df.columns:
                        corrected_columns_order.append(alt_col)
                    else:
                        corrected_columns_order.append(col)
                else:
                    corrected_columns_order.append(col)
            else:
                corrected_columns_order.append(col)
        columns_order = corrected_columns_order

        final_columns = columns_order.copy()
        if theme_dummies is not None:
            for theme_col in sorted(theme_dummies.columns):
                if theme_col not in final_columns:
                    final_columns.append(theme_col)

        for col in final_columns:
            if col not in final_df.columns:
                final_df[col] = np.nan

        final_df = final_df[final_columns]

        if 'í…Œë§ˆ_ì¤‘ë³µ_ëª©ë¡' in final_df.columns:
            final_df = final_df.drop(columns=['í…Œë§ˆ_ì¤‘ë³µ_ëª©ë¡'])

        final_df.to_csv(output_filename, index=False, encoding='utf-8-sig')

        print(f"âœ… ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {output_filename} (ì´ {len(final_df)} í–‰)")

    except Exception as e:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    print("\n" + "=" * 40)
    print("  â­ ì£¼ë„ì£¼-í›„ë³´ì£¼ ë°ì´í„°ì…‹ êµ¬ì¶• ì‹œì‘ (í•˜ì´ë¸Œë¦¬ë“œ) â­")
    print("=" * 40)
    cuda_available = torch.cuda.is_available()
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {cuda_available}")
    if cuda_available:
        print(f"í˜„ì¬ GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
        print(f"PyTorch CUDA ë²„ì „: {torch.version.cuda}")

    final_output_dir = "leader_follower_datasets"
    os.makedirs(final_output_dir, exist_ok=True)

    final_output_filename = os.path.join(final_output_dir, "final_dataset_combined.csv")
    log_file_path = os.path.join(final_output_dir, "processed_log.txt")
    print(f"--- ìµœì¢… ë°ì´í„°ì…‹: '{final_output_filename}' ---")
    print(f"--- ì§„í–‰ ìƒí™© ë¡œê·¸: '{log_file_path}' ---")

    load_theme_db("naver_themes_bs4.csv")
    if not THEME_DB:
        print("âŒ í…Œë§ˆ DBê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. 'naver_themes_bs4.csv' íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit()

    tokenizer, model = load_sentiment_model()

    input_filename = input("ìŠ¤í¬ë˜í•‘ ëª©ë¡ CSV íŒŒì¼ëª… ì…ë ¥ (ì˜ˆ: input_list.csv): ")
    keyword_date_list = read_keywords_from_file(input_filename)
    if not keyword_date_list:
        print("âŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit()

    all_events_rows = []
    processed_events = set()

    if os.path.exists(log_file_path):
        try:
            with open(log_file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    processed_events.add(line.strip())
            print(f"--- [ì´ì–´í•˜ê¸°] {len(processed_events)}ê°œì˜ ì´ë²¤íŠ¸ë¥¼ ë¡œê·¸ì—ì„œ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ---")
        except Exception as e:
            print(f"âš ï¸ ë¡œê·¸ íŒŒì¼({log_file_path}) ì½ê¸° ì˜¤ë¥˜: {e}. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            processed_events = set()

    if os.path.exists(final_output_filename) and processed_events:
        print(f"--- [ì´ì–´í•˜ê¸°] '{final_output_filename}'ì—ì„œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤... ---")
        try:
            existing_df = pd.read_csv(final_output_filename)
            cols_to_load = [col for col in existing_df.columns if not col.startswith('í…Œë§ˆ_')]
            if 'í…Œë§ˆ_ì¤‘ë³µ_ëª©ë¡' not in cols_to_load and 'í…Œë§ˆ_ì¤‘ë³µ_ê°œìˆ˜' in cols_to_load:
                cols_to_load.insert(cols_to_load.index('í…Œë§ˆ_ì¤‘ë³µ_ê°œìˆ˜') + 1, 'í…Œë§ˆ_ì¤‘ë³µ_ëª©ë¡')

            for base_col in ['ì£¼ë„ì£¼', 'T_Date', 'í›„ë³´ì£¼', 'í…Œë§ˆ_ì¤‘ë³µ_ëª©ë¡']:
                if base_col not in existing_df.columns:
                    existing_df[base_col] = np.nan

            base_df = existing_df[cols_to_load]
            all_events_rows = base_df.to_dict('records')
            print(f"âœ… {len(all_events_rows)}ê°œì˜ ê¸°ì¡´ í–‰ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ---")
        except Exception as e:
            print(f"âš ï¸ ê²½ê³ : ê¸°ì¡´ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            all_events_rows = []
            processed_events = set()
    else:
        if processed_events:
            print(f"--- [ê²½ê³ ] ë¡œê·¸ íŒŒì¼ì€ ìˆìœ¼ë‚˜ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ---")
            processed_events = set()

    print("\n--- 1íšŒìš© ì›¹ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘... (ì¢…ë£Œ ì‹œê¹Œì§€ ì¬ì‚¬ìš©) ---")
    driver = initialize_driver()
    if driver is None:
        print("âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ì›¹ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit()

    print(f"\n--- ì´ {len(keyword_date_list)}ê°œì˜ ì£¼ë„ì£¼ ì´ë²¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤ ---")

    total_start_time = time.time()
    events_processed_count = 0

    try:
        for keyword, end_date_ymd in keyword_date_list:

            event_start_time = time.time()

            current_event_rows = []

            event_key = f"{keyword},{end_date_ymd}"
            if event_key in processed_events:
                print(f"--- [SKIP] {keyword} ({end_date_ymd})ëŠ”(ì€) ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ---")
                continue

            try:
                end_date_dt = datetime.strptime(end_date_ymd, '%Y%m%d')
                start_date_dt = end_date_dt - timedelta(days=2)
                start_date_ymd = start_date_dt.strftime('%Y%m%d')

                print(f"\n" + "=" * 50)
                print(f"  [1/2] ì£¼ë„ì£¼ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹œì‘: {keyword} (Tì¼: {end_date_ymd})")
                print(f"=" * 50)

                base_event_data = {'ì£¼ë„ì£¼': keyword, 'T_Date': end_date_ymd}

                news_results = scrape_naver_news_only_full_xpath(
                    driver, keyword, start_date_ymd, end_date_ymd, max_count=10
                )

                news_filename = None
                if news_results:
                    news_filename = save_news_to_file(keyword, start_date_ymd, end_date_ymd, news_results)
                base_event_data['ë‰´ìŠ¤_íŒŒì¼_ê²½ë¡œ'] = news_filename

                combined_text = ""
                if news_results:
                    combined_text = "".join(
                        f"[ê¸°ì‚¬ {i + 1}] ì œëª©: {news['title']}\n---\n" for i, news in
                        enumerate(news_results))
                else:
                    print("âš ï¸ ë‰´ìŠ¤ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                finbert_text = ""
                if news_results:
                    finbert_text = "".join(
                        f"[ê¸°ì‚¬ {i + 1}] ì œëª©: {news['title']}\në‚´ìš©: {news['summary']}\n---\n" for i, news in
                        enumerate(news_results))

                leader_themes = STOCK_TO_THEMES_DB.get(keyword)
                if not leader_themes:
                    print(f"âŒ {keyword}ê°€(ì´) ë¡œë“œëœ í…Œë§ˆ DBì— ì—†ìŠµë‹ˆë‹¤. ì´ë²¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                event_theme, event_scale = call_openai_for_theme(combined_text, keyword, leader_themes)

                if not event_theme:
                    print(f"âŒ OpenAIê°€ ë‰´ìŠ¤ì™€ ì¼ì¹˜í•˜ëŠ” í…Œë§ˆë¥¼ ë¶„ë¥˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ë²¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                print(f"--- OpenAI í…Œë§ˆ ë¶„ë¥˜ ì™„ë£Œ: '{event_theme}' (ê·œëª¨: {event_scale}) ---")
                base_event_data['í…Œë§ˆ'] = event_theme
                base_event_data['ë‰´ìŠ¤ê¸°ì‚¬_ê·œëª¨ì ì§€ìˆ˜'] = event_scale

                print(f"--- [2/2] ì£¼ë„ì£¼({keyword}) í”¼ì²˜ ê³„ì‚° ì¤‘... ---")
                leader_ticker = get_ticker_by_name(keyword, end_date_ymd)
                base_event_data['ì£¼ë„ì£¼_Ticker'] = leader_ticker
                if not leader_ticker:
                    print(f"âŒ ì£¼ë„ì£¼ Tickerë¥¼ ì°¾ì§€ ëª»í•´ ì´ë²¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                leader_features = calculate_leader_features(leader_ticker, end_date_ymd)
                if leader_features:
                    base_event_data.update(leader_features)
                    print(f"âœ… ì£¼ë„ì£¼ í”¼ì²˜ ê³„ì‚° ì™„ë£Œ: {keyword} (Ticker: {leader_ticker})")
                else:
                    print(f"âŒ ì£¼ë„ì£¼ í”¼ì²˜ ê³„ì‚° ì‹¤íŒ¨. ì´ë²¤íŠ¸ë¥¼ ê±´ë„ˆí‚µë‹ˆë‹¤.")
                    continue

                print(f"--- í™•ì •ëœ í…Œë§ˆ [{event_theme}] ë¡œ í›„ë³´ì£¼ íƒìƒ‰... ---")
                followers = get_followers(event_theme, keyword)
                if not followers:
                    print(f"âš ï¸ {event_theme} í…Œë§ˆì— í›„ë³´ì£¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                for follower_name in followers:
                    try:
                        follower_ticker = get_ticker_by_name(follower_name, end_date_ymd)
                        if not follower_ticker:
                            continue

                        row_data = base_event_data.copy()
                        row_data['í›„ë³´ì£¼'] = follower_name
                        row_data['í›„ë³´ì£¼_Ticker'] = follower_ticker

                        theme_keyword_simple = event_theme.split('(')[0].strip()
                        filter_keywords = [keyword, follower_name, theme_keyword_simple]

                        sentiment_score, sentence_score_list = calculate_finbert_sentiment(
                            finbert_text,
                            filter_keywords,
                            tokenizer,
                            model
                        )

                        print(f"\n--- FinBert ë¶„ì„ ({keyword} -> {follower_name}) ---")
                        if sentence_score_list:
                            print(f"  [ë¶„ì„ ëŒ€ìƒ ë¬¸ì¥ ({len(sentence_score_list)}ê°œ)]")
                            for sent, score in sentence_score_list:
                                print(f"    - [ì ìˆ˜: {score:+.4f}] {sent}")
                            print(f"  [ìµœì¢… í‰ê·  ì ìˆ˜]: {sentiment_score:.4f}")
                            save_sentiment_to_csv(
                                keyword, follower_name, start_date_ymd, end_date_ymd,
                                sentence_score_list, sentiment_score
                            )
                        else:
                            print(f"  [ë¶„ì„ ëŒ€ìƒ ë¬¸ì¥ ì—†ìŒ] (í‚¤ì›Œë“œ: {', '.join(filter_keywords)})")

                        row_data['ë‰´ìŠ¤ê¸°ì‚¬_ê°ì„±ì§€ìˆ˜'] = sentiment_score

                        tech_f, funda_f, actual_t_day_ts, t_day_close = calculate_follower_features(
                            follower_ticker, end_date_ymd
                        )
                        if tech_f: row_data.update(tech_f)
                        if funda_f: row_data.update(funda_f)

                        target_label = get_next_day_label(follower_ticker, actual_t_day_ts, t_day_close)
                        row_data['Target'] = target_label

                        correlation = get_correlation(leader_ticker, follower_ticker, end_date_ymd)
                        row_data['ìƒê´€ê´€ê³„_ì§€ìˆ˜'] = correlation

                        count, theme_list = get_theme_overlap(follower_name)
                        row_data['í…Œë§ˆ_ì¤‘ë³µ_ê°œìˆ˜'] = count
                        row_data['í…Œë§ˆ_ì¤‘ë³µ_ëª©ë¡'] = theme_list

                        current_event_rows.append(row_data)

                    except Exception as e:
                        print(f"âŒ í›„ë³´ì£¼ '{follower_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue

                if current_event_rows:
                    all_events_rows.extend(current_event_rows)
                    print(f"âœ… {keyword}({end_date_ymd}) ì´ë²¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ. ëˆ„ì  ë°ì´í„°: {len(all_events_rows)} í–‰.")

                    save_intermediate_dataset(all_events_rows, final_output_filename)

                    with open(log_file_path, 'a', encoding='utf-8') as f:
                        f.write(f"{event_key}\n")

                    processed_events.add(event_key)

                else:
                    print(f"--- {keyword}({end_date_ymd}) ì´ë²¤íŠ¸ì—ì„œ ì²˜ë¦¬ëœ í›„ë³´ì£¼ê°€ ì—†ì–´, ì¶”ê°€ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ---")

            except Exception as e:
                print(f"âŒ '{keyword}' ì´ë²¤íŠ¸ ì „ì²´ ì‘ì—… ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")

            event_elapsed_time = time.time() - event_start_time
            events_processed_count += 1
            print(f"--- â±ï¸  {keyword}({end_date_ymd}) ì²˜ë¦¬ ì†Œìš” ì‹œê°„: {event_elapsed_time:.2f}ì´ˆ ---")

            total_completed = len(processed_events) + events_processed_count
            remaining_events = len(keyword_date_list) - total_completed

            if events_processed_count > 0:
                avg_time_per_event = (time.time() - total_start_time) / events_processed_count
                eta_seconds = avg_time_per_event * remaining_events
                eta_minutes = eta_seconds / 60
                print(
                    f"--- ğŸ“Š ì§„í–‰ ìƒí™©: {total_completed}/{len(keyword_date_list)} | ë‚¨ì€ ì˜ˆìƒ ì‹œê°„: {eta_minutes:.1f}ë¶„ ({eta_seconds:.0f}ì´ˆ) ---")

    finally:
        if driver:
            driver.quit()
            print("\n--- ëª¨ë“  ì‘ì—… ì™„ë£Œ. ì›¹ ë“œë¼ì´ë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ---")

    if not all_events_rows:
        print("\n--- ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ì–´ ìµœì¢… CSV íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ---")
    else:
        total_elapsed_time = time.time() - total_start_time
        total_minutes = total_elapsed_time / 60

        print(f"\n" + "=" * 50)
        print(f"  ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ (ì´ë²ˆ ì‹¤í–‰ì—ì„œ {events_processed_count}ê°œ ì‹ ê·œ ì´ë²¤íŠ¸ ì²˜ë¦¬)")
        print(f"  ì´ë²ˆ ì‹¤í–‰ ì´ ì†Œìš” ì‹œê°„: {total_minutes:.2f}ë¶„ ({total_elapsed_time:.2f}ì´ˆ)")
        print(f"  ìµœì¢… íŒŒì¼: {final_output_filename}")
        print(f"  ì´ {len(all_events_rows)}ê°œì˜ (ì£¼ë„ì£¼-í›„ë³´ì£¼) ê´€ê³„ê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"=" * 50)

    print("\n=== ëª¨ë“  ì‘ì—… ì™„ë£Œ ===")